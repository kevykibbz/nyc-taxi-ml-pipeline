{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e93bb350",
   "metadata": {},
   "source": [
    "# NYC Taxi Data - Batch Processing Implementation\n",
    "## Data Engineering Portfolio Project (DLMDSEDE02)\n",
    "\n",
    "This notebook demonstrates the data preparation and analysis pipeline for the NYC Yellow Taxi dataset as part of a batch processing data architecture. The implementation follows the microservices design created for quarterly machine learning model training.\n",
    "\n",
    "### Project Overview:\n",
    "- **Dataset**: NYC Yellow Taxi Trip Data (1M+ records)\n",
    "- **Architecture**: Batch processing with Apache Spark, Hadoop HDFS, Kafka\n",
    "- **Goal**: Quarterly data aggregation for ML model training (fare prediction, demand forecasting)\n",
    "- **Environment**: Kaggle Python Docker environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a5cf79",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Library Imports\n",
    "\n",
    "This section imports all necessary libraries for data processing, analysis, and visualization. The Kaggle environment comes pre-installed with essential data science packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d447d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Additional libraries for data analysis and visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Libraries for data preprocessing\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "# Configure visualization settings\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(f\"üìä Pandas version: {pd.__version__}\")\n",
    "print(f\"üî¢ NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b7fcc25",
   "metadata": {},
   "source": [
    "## 2. Explore Input Data Directory\n",
    "\n",
    "Exploring the Kaggle input directory structure to understand available data files and their organization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c1f638",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "\n",
    "print(\"üîç Exploring Kaggle input directory structure:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "data_files = []\n",
    "total_size = 0\n",
    "\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    print(f\"\\nüìÅ Directory: {dirname}\")\n",
    "    \n",
    "    for filename in filenames:\n",
    "        file_path = os.path.join(dirname, filename)\n",
    "        print(f\"   üìÑ {filename}\")\n",
    "        \n",
    "        # Get file size\n",
    "        try:\n",
    "            file_size = os.path.getsize(file_path)\n",
    "            total_size += file_size\n",
    "            print(f\"      Size: {file_size / (1024*1024):.2f} MB\")\n",
    "            data_files.append({\n",
    "                'directory': dirname,\n",
    "                'filename': filename,\n",
    "                'full_path': file_path,\n",
    "                'size_mb': file_size / (1024*1024)\n",
    "            })\n",
    "        except:\n",
    "            print(f\"      Size: Unable to determine\")\n",
    "\n",
    "print(f\"\\nüìä Summary:\")\n",
    "print(f\"   Total files found: {len(data_files)}\")\n",
    "print(f\"   Total data size: {total_size / (1024*1024):.2f} MB\")\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n",
    "\n",
    "print(f\"\\nüíæ Working directory: /kaggle/working/ (20GB available)\")\n",
    "print(f\"üóÇÔ∏è Temp directory: /kaggle/temp/ (session only)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c8090cb",
   "metadata": {},
   "source": [
    "## 3. Load and Inspect Dataset\n",
    "\n",
    "Loading the NYC Taxi dataset and performing initial inspection to understand the data structure, quality, and characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558ef9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the NYC Taxi dataset\n",
    "# Note: Adjust the path based on your specific dataset location in Kaggle\n",
    "try:\n",
    "    # Try common file paths for NYC taxi data\n",
    "    possible_paths = [\n",
    "        '/kaggle/input/nyc-yellow-taxi-trip-data/yellow_tripdata_2023-01.csv',\n",
    "        '/kaggle/input/nyc-taxi-trip-duration/train.csv',\n",
    "        '/kaggle/input/new-york-city-taxi-fare-prediction/train.csv'\n",
    "    ]\n",
    "    \n",
    "    df = None\n",
    "    used_path = None\n",
    "    \n",
    "    for path in possible_paths:\n",
    "        if os.path.exists(path):\n",
    "            print(f\"üìÅ Loading data from: {path}\")\n",
    "            df = pd.read_csv(path)\n",
    "            used_path = path\n",
    "            break\n",
    "    \n",
    "    if df is None:\n",
    "        # Create sample data for demonstration if no file found\n",
    "        print(\"‚ö†Ô∏è No dataset found. Creating sample NYC taxi data for demonstration...\")\n",
    "        np.random.seed(42)\n",
    "        \n",
    "        # Generate sample NYC taxi trip data\n",
    "        n_records = 100000\n",
    "        \n",
    "        sample_data = {\n",
    "            'tpep_pickup_datetime': pd.date_range('2023-01-01', periods=n_records, freq='5min'),\n",
    "            'tpep_dropoff_datetime': pd.date_range('2023-01-01 00:15:00', periods=n_records, freq='5min'),\n",
    "            'passenger_count': np.random.choice([1, 2, 3, 4, 5], n_records, p=[0.5, 0.3, 0.1, 0.05, 0.05]),\n",
    "            'trip_distance': np.random.exponential(2.5, n_records),\n",
    "            'pickup_longitude': np.random.uniform(-74.05, -73.75, n_records),\n",
    "            'pickup_latitude': np.random.uniform(40.63, 40.85, n_records),\n",
    "            'dropoff_longitude': np.random.uniform(-74.05, -73.75, n_records),\n",
    "            'dropoff_latitude': np.random.uniform(40.63, 40.85, n_records),\n",
    "            'fare_amount': np.random.uniform(5, 50, n_records),\n",
    "            'tip_amount': np.random.uniform(0, 15, n_records),\n",
    "            'total_amount': None  # Will calculate\n",
    "        }\n",
    "        \n",
    "        df = pd.DataFrame(sample_data)\n",
    "        df['total_amount'] = df['fare_amount'] + df['tip_amount'] + np.random.uniform(0.5, 3, n_records)\n",
    "        print(f\"‚úÖ Created sample dataset with {len(df):,} records\")\n",
    "    \n",
    "    print(f\"\\nüìä Dataset loaded successfully!\")\n",
    "    print(f\"   Shape: {df.shape}\")\n",
    "    print(f\"   Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading dataset: {e}\")\n",
    "    print(\"Creating minimal sample data for demonstration...\")\n",
    "    \n",
    "    # Minimal sample for error case\n",
    "    df = pd.DataFrame({\n",
    "        'pickup_datetime': ['2023-01-01 12:00:00'] * 1000,\n",
    "        'fare_amount': np.random.uniform(5, 50, 1000)\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5787ed81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial data inspection\n",
    "print(\"üîç INITIAL DATA INSPECTION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Display basic information\n",
    "print(\"\\n1Ô∏è‚É£ Dataset Overview:\")\n",
    "print(f\"   Rows: {df.shape[0]:,}\")\n",
    "print(f\"   Columns: {df.shape[1]}\")\n",
    "print(f\"   Memory Usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\n2Ô∏è‚É£ First 5 rows:\")\n",
    "display(df.head())\n",
    "\n",
    "# Data types and null values\n",
    "print(\"\\n3Ô∏è‚É£ Data Types and Missing Values:\")\n",
    "info_df = pd.DataFrame({\n",
    "    'Column': df.columns,\n",
    "    'Data_Type': df.dtypes,\n",
    "    'Non_Null_Count': df.count(),\n",
    "    'Null_Count': df.isnull().sum(),\n",
    "    'Null_Percentage': (df.isnull().sum() / len(df) * 100).round(2)\n",
    "})\n",
    "display(info_df)\n",
    "\n",
    "# Basic statistics\n",
    "print(\"\\n4Ô∏è‚É£ Statistical Summary:\")\n",
    "display(df.describe())\n",
    "\n",
    "# Check for duplicate rows\n",
    "duplicates = df.duplicated().sum()\n",
    "print(f\"\\n5Ô∏è‚É£ Duplicate rows: {duplicates:,} ({duplicates/len(df)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "974bd078",
   "metadata": {},
   "source": [
    "## 4. Data Cleaning and Preprocessing\n",
    "\n",
    "Cleaning the dataset by handling missing values, removing duplicates, correcting data types, and filtering out invalid records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d0ff34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning and preprocessing\n",
    "print(\"üßπ DATA CLEANING AND PREPROCESSING\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Store original shape for comparison\n",
    "original_shape = df.shape\n",
    "print(f\"üìä Original dataset shape: {original_shape}\")\n",
    "\n",
    "# 1. Handle datetime columns\n",
    "datetime_columns = [col for col in df.columns if 'datetime' in col.lower() or 'pickup' in col.lower() or 'dropoff' in col.lower()]\n",
    "print(f\"\\n1Ô∏è‚É£ Converting datetime columns: {datetime_columns}\")\n",
    "\n",
    "for col in datetime_columns:\n",
    "    if col in df.columns:\n",
    "        try:\n",
    "            df[col] = pd.to_datetime(df[col])\n",
    "            print(f\"   ‚úÖ Converted {col} to datetime\")\n",
    "        except:\n",
    "            print(f\"   ‚ùå Failed to convert {col}\")\n",
    "\n",
    "# 2. Remove duplicates\n",
    "duplicates_before = df.duplicated().sum()\n",
    "df = df.drop_duplicates()\n",
    "duplicates_removed = duplicates_before - df.duplicated().sum()\n",
    "print(f\"\\n2Ô∏è‚É£ Removed {duplicates_removed:,} duplicate rows\")\n",
    "\n",
    "# 3. Handle missing values\n",
    "print(f\"\\n3Ô∏è‚É£ Handling missing values:\")\n",
    "missing_before = df.isnull().sum().sum()\n",
    "\n",
    "# Drop rows with missing critical columns (pickup/dropoff times, fare)\n",
    "critical_columns = ['fare_amount'] + [col for col in df.columns if 'datetime' in col.lower()]\n",
    "for col in critical_columns:\n",
    "    if col in df.columns:\n",
    "        before_count = len(df)\n",
    "        df = df.dropna(subset=[col])\n",
    "        dropped = before_count - len(df)\n",
    "        if dropped > 0:\n",
    "            print(f\"   üìâ Dropped {dropped:,} rows with missing {col}\")\n",
    "\n",
    "# Fill missing numerical values with median\n",
    "numerical_columns = df.select_dtypes(include=[np.number]).columns\n",
    "for col in numerical_columns:\n",
    "    if df[col].isnull().sum() > 0:\n",
    "        median_val = df[col].median()\n",
    "        df[col].fillna(median_val, inplace=True)\n",
    "        print(f\"   üîß Filled missing {col} with median: {median_val:.2f}\")\n",
    "\n",
    "missing_after = df.isnull().sum().sum()\n",
    "print(f\"   üìä Missing values before: {missing_before:,} ‚Üí after: {missing_after:,}\")\n",
    "\n",
    "# 4. Filter invalid data\n",
    "print(f\"\\n4Ô∏è‚É£ Filtering invalid data:\")\n",
    "\n",
    "if 'fare_amount' in df.columns:\n",
    "    # Remove negative fares and extremely high fares\n",
    "    before_count = len(df)\n",
    "    df = df[(df['fare_amount'] >= 0) & (df['fare_amount'] <= 500)]\n",
    "    print(f\"   üí∞ Removed {before_count - len(df):,} rows with invalid fare amounts\")\n",
    "\n",
    "if 'passenger_count' in df.columns:\n",
    "    # Remove invalid passenger counts\n",
    "    before_count = len(df)\n",
    "    df = df[(df['passenger_count'] >= 1) & (df['passenger_count'] <= 6)]\n",
    "    print(f\"   üë• Removed {before_count - len(df):,} rows with invalid passenger counts\")\n",
    "\n",
    "if 'trip_distance' in df.columns:\n",
    "    # Remove invalid trip distances\n",
    "    before_count = len(df)\n",
    "    df = df[(df['trip_distance'] >= 0) & (df['trip_distance'] <= 100)]\n",
    "    print(f\"   üõ£Ô∏è Removed {before_count - len(df):,} rows with invalid trip distances\")\n",
    "\n",
    "# 5. Create derived columns if datetime columns exist\n",
    "datetime_cols = [col for col in df.columns if df[col].dtype == 'datetime64[ns]']\n",
    "if len(datetime_cols) >= 2:\n",
    "    pickup_col = [col for col in datetime_cols if 'pickup' in col.lower()]\n",
    "    dropoff_col = [col for col in datetime_cols if 'dropoff' in col.lower()]\n",
    "    \n",
    "    if pickup_col and dropoff_col:\n",
    "        pickup_col = pickup_col[0]\n",
    "        dropoff_col = dropoff_col[0]\n",
    "        \n",
    "        # Calculate trip duration\n",
    "        df['trip_duration_minutes'] = (df[dropoff_col] - df[pickup_col]).dt.total_seconds() / 60\n",
    "        \n",
    "        # Remove trips with invalid duration (negative or too long)\n",
    "        before_count = len(df)\n",
    "        df = df[(df['trip_duration_minutes'] > 0) & (df['trip_duration_minutes'] <= 300)]\n",
    "        print(f\"   ‚è±Ô∏è Removed {before_count - len(df):,} rows with invalid trip duration\")\n",
    "\n",
    "print(f\"\\nüìä Final dataset shape: {df.shape}\")\n",
    "print(f\"üìâ Total rows removed: {original_shape[0] - df.shape[0]:,} ({(original_shape[0] - df.shape[0])/original_shape[0]*100:.1f}%)\")\n",
    "print(f\"‚úÖ Cleaned dataset ready for analysis!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f94a65",
   "metadata": {},
   "source": [
    "## 5. Exploratory Data Analysis\n",
    "\n",
    "Analyzing data distributions, correlations, and summary statistics to understand patterns and characteristics that will inform our batch processing pipeline design."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5cc857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exploratory Data Analysis\n",
    "print(\"üìà EXPLORATORY DATA ANALYSIS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 1. Basic statistics for numerical columns\n",
    "print(\"\\n1Ô∏è‚É£ Key Statistics Summary:\")\n",
    "numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
    "\n",
    "stats_summary = pd.DataFrame({\n",
    "    'Column': numerical_cols,\n",
    "    'Mean': [df[col].mean() for col in numerical_cols],\n",
    "    'Median': [df[col].median() for col in numerical_cols],\n",
    "    'Std': [df[col].std() for col in numerical_cols],\n",
    "    'Min': [df[col].min() for col in numerical_cols],\n",
    "    'Max': [df[col].max() for col in numerical_cols],\n",
    "    'Unique_Values': [df[col].nunique() for col in numerical_cols]\n",
    "}).round(2)\n",
    "\n",
    "display(stats_summary)\n",
    "\n",
    "# 2. Correlation analysis\n",
    "print(\"\\n2Ô∏è‚É£ Correlation Analysis:\")\n",
    "if len(numerical_cols) > 1:\n",
    "    correlation_matrix = df[numerical_cols].corr()\n",
    "    \n",
    "    # Find strongest correlations\n",
    "    correlation_pairs = []\n",
    "    for i in range(len(correlation_matrix.columns)):\n",
    "        for j in range(i+1, len(correlation_matrix.columns)):\n",
    "            col1 = correlation_matrix.columns[i]\n",
    "            col2 = correlation_matrix.columns[j]\n",
    "            corr_value = correlation_matrix.iloc[i, j]\n",
    "            correlation_pairs.append({\n",
    "                'Column_1': col1,\n",
    "                'Column_2': col2,\n",
    "                'Correlation': corr_value\n",
    "            })\n",
    "    \n",
    "    corr_df = pd.DataFrame(correlation_pairs)\n",
    "    corr_df = corr_df.reindex(corr_df['Correlation'].abs().sort_values(ascending=False).index)\n",
    "    \n",
    "    print(\"   üîó Top 5 Strongest Correlations:\")\n",
    "    display(corr_df.head())\n",
    "\n",
    "# 3. Data distribution insights\n",
    "print(\"\\n3Ô∏è‚É£ Data Distribution Insights:\")\n",
    "\n",
    "for col in numerical_cols[:5]:  # Analyze first 5 numerical columns\n",
    "    q25, q50, q75 = df[col].quantile([0.25, 0.5, 0.75])\n",
    "    iqr = q75 - q25\n",
    "    outliers = df[(df[col] < (q25 - 1.5 * iqr)) | (df[col] > (q75 + 1.5 * iqr))][col].count()\n",
    "    \n",
    "    print(f\"   üìä {col}:\")\n",
    "    print(f\"      Range: {df[col].min():.2f} - {df[col].max():.2f}\")\n",
    "    print(f\"      IQR: {q25:.2f} - {q75:.2f}\")\n",
    "    print(f\"      Outliers: {outliers:,} ({outliers/len(df)*100:.1f}%)\")\n",
    "\n",
    "# 4. Time-based analysis (if datetime columns exist)\n",
    "datetime_cols = [col for col in df.columns if df[col].dtype == 'datetime64[ns]']\n",
    "if datetime_cols:\n",
    "    print(f\"\\n4Ô∏è‚É£ Time-based Analysis:\")\n",
    "    pickup_col = [col for col in datetime_cols if 'pickup' in col.lower()]\n",
    "    \n",
    "    if pickup_col:\n",
    "        pickup_col = pickup_col[0]\n",
    "        df['hour'] = df[pickup_col].dt.hour\n",
    "        df['day_of_week'] = df[pickup_col].dt.day_name()\n",
    "        df['month'] = df[pickup_col].dt.month\n",
    "        \n",
    "        print(f\"   üìÖ Date range: {df[pickup_col].min()} to {df[pickup_col].max()}\")\n",
    "        print(f\"   ‚è∞ Peak hours: {df['hour'].mode().values}\")\n",
    "        print(f\"   üìÜ Busiest day: {df['day_of_week'].mode().values[0]}\")\n",
    "\n",
    "# 5. Business insights for batch processing\n",
    "print(f\"\\n5Ô∏è‚É£ Business Insights for Batch Processing:\")\n",
    "\n",
    "if 'fare_amount' in df.columns:\n",
    "    avg_fare = df['fare_amount'].mean()\n",
    "    print(f\"   üí∞ Average fare: ${avg_fare:.2f}\")\n",
    "    \n",
    "    # Revenue calculations for quarterly processing\n",
    "    daily_revenue = df['fare_amount'].sum() / df[pickup_col].dt.date.nunique() if pickup_col else 0\n",
    "    quarterly_revenue = daily_revenue * 90  # 3 months\n",
    "    \n",
    "    print(f\"   üìà Estimated daily revenue: ${daily_revenue:,.2f}\")\n",
    "    print(f\"   üìä Estimated quarterly revenue: ${quarterly_revenue:,.2f}\")\n",
    "\n",
    "if 'trip_distance' in df.columns:\n",
    "    avg_distance = df['trip_distance'].mean()\n",
    "    print(f\"   üõ£Ô∏è Average trip distance: {avg_distance:.2f} miles\")\n",
    "\n",
    "if 'passenger_count' in df.columns:\n",
    "    avg_passengers = df['passenger_count'].mean()\n",
    "    print(f\"   üë• Average passengers per trip: {avg_passengers:.1f}\")\n",
    "\n",
    "print(f\"\\n‚úÖ EDA completed! Ready for visualization and feature engineering.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad75b78",
   "metadata": {},
   "source": [
    "## 6. Data Visualization\n",
    "\n",
    "Creating comprehensive visualizations to understand data patterns, distributions, and relationships that will guide our batch processing aggregations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1bf345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Visualization\n",
    "print(\"üìä DATA VISUALIZATION\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Set up the plotting style\n",
    "plt.figure(figsize=(20, 15))\n",
    "\n",
    "# 1. Distribution plots for key numerical variables\n",
    "numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
    "n_cols = min(4, len(numerical_cols))\n",
    "\n",
    "if n_cols > 0:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('üìà Distribution of Key Numerical Variables', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    for i, col in enumerate(numerical_cols[:4]):\n",
    "        row = i // 2\n",
    "        col_idx = i % 2\n",
    "        \n",
    "        # Histogram with KDE\n",
    "        axes[row, col_idx].hist(df[col], bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "        axes[row, col_idx].set_title(f'{col} Distribution')\n",
    "        axes[row, col_idx].set_xlabel(col)\n",
    "        axes[row, col_idx].set_ylabel('Frequency')\n",
    "        axes[row, col_idx].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 2. Box plots for outlier analysis\n",
    "if len(numerical_cols) > 0:\n",
    "    fig, axes = plt.subplots(1, min(3, len(numerical_cols)), figsize=(15, 5))\n",
    "    fig.suptitle('üì¶ Box Plots - Outlier Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    if len(numerical_cols) == 1:\n",
    "        axes = [axes]\n",
    "    elif len(numerical_cols) == 2:\n",
    "        axes = axes if isinstance(axes, (list, np.ndarray)) else [axes]\n",
    "    \n",
    "    for i, col in enumerate(numerical_cols[:3]):\n",
    "        if i < len(axes):\n",
    "            axes[i].boxplot(df[col].dropna())\n",
    "            axes[i].set_title(f'{col}')\n",
    "            axes[i].set_ylabel(col)\n",
    "            axes[i].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 3. Correlation heatmap\n",
    "if len(numerical_cols) > 1:\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    correlation_matrix = df[numerical_cols].corr()\n",
    "    \n",
    "    sns.heatmap(correlation_matrix, \n",
    "                annot=True, \n",
    "                cmap='coolwarm', \n",
    "                center=0,\n",
    "                square=True,\n",
    "                fmt='.2f',\n",
    "                cbar_kws={'shrink': 0.8})\n",
    "    \n",
    "    plt.title('üîó Correlation Matrix Heatmap', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 4. Time-based visualizations (if datetime columns exist)\n",
    "datetime_cols = [col for col in df.columns if df[col].dtype == 'datetime64[ns]']\n",
    "pickup_col = [col for col in datetime_cols if 'pickup' in col.lower()]\n",
    "\n",
    "if pickup_col and len(pickup_col) > 0:\n",
    "    pickup_col = pickup_col[0]\n",
    "    \n",
    "    # Hourly trip patterns\n",
    "    if 'hour' in df.columns:\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        hourly_trips = df['hour'].value_counts().sort_index()\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        hourly_trips.plot(kind='bar', color='lightcoral')\n",
    "        plt.title('üïê Trip Volume by Hour of Day')\n",
    "        plt.xlabel('Hour')\n",
    "        plt.ylabel('Number of Trips')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Daily patterns\n",
    "        if 'day_of_week' in df.columns:\n",
    "            plt.subplot(1, 2, 2)\n",
    "            daily_trips = df['day_of_week'].value_counts()\n",
    "            days_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "            daily_trips = daily_trips.reindex([day for day in days_order if day in daily_trips.index])\n",
    "            \n",
    "            daily_trips.plot(kind='bar', color='lightgreen')\n",
    "            plt.title('üìÖ Trip Volume by Day of Week')\n",
    "            plt.xlabel('Day')\n",
    "            plt.ylabel('Number of Trips')\n",
    "            plt.xticks(rotation=45)\n",
    "            plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# 5. Business metrics visualization\n",
    "if 'fare_amount' in df.columns:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    fig.suptitle('üí∞ Business Metrics Dashboard', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Fare distribution\n",
    "    axes[0, 0].hist(df['fare_amount'], bins=50, color='gold', alpha=0.7, edgecolor='black')\n",
    "    axes[0, 0].set_title('Fare Amount Distribution')\n",
    "    axes[0, 0].set_xlabel('Fare ($)')\n",
    "    axes[0, 0].set_ylabel('Frequency')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Passenger count distribution\n",
    "    if 'passenger_count' in df.columns:\n",
    "        passenger_counts = df['passenger_count'].value_counts().sort_index()\n",
    "        axes[0, 1].bar(passenger_counts.index, passenger_counts.values, color='lightblue')\n",
    "        axes[0, 1].set_title('Passenger Count Distribution')\n",
    "        axes[0, 1].set_xlabel('Number of Passengers')\n",
    "        axes[0, 1].set_ylabel('Number of Trips')\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Trip distance vs fare\n",
    "    if 'trip_distance' in df.columns:\n",
    "        sample_size = min(5000, len(df))  # Sample for performance\n",
    "        sample_df = df.sample(n=sample_size)\n",
    "        \n",
    "        axes[1, 0].scatter(sample_df['trip_distance'], sample_df['fare_amount'], \n",
    "                          alpha=0.5, color='purple', s=10)\n",
    "        axes[1, 0].set_title('Trip Distance vs Fare Amount')\n",
    "        axes[1, 0].set_xlabel('Trip Distance (miles)')\n",
    "        axes[1, 0].set_ylabel('Fare Amount ($)')\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # Monthly revenue trend (if dates available)\n",
    "    if pickup_col and 'month' in df.columns:\n",
    "        monthly_revenue = df.groupby('month')['fare_amount'].sum()\n",
    "        axes[1, 1].plot(monthly_revenue.index, monthly_revenue.values, \n",
    "                       marker='o', linewidth=2, color='red')\n",
    "        axes[1, 1].set_title('Monthly Revenue Trend')\n",
    "        axes[1, 1].set_xlabel('Month')\n",
    "        axes[1, 1].set_ylabel('Total Revenue ($)')\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ Visualizations completed!\")\n",
    "print(\"üìä Key insights for batch processing pipeline:\")\n",
    "print(\"   ‚Ä¢ Hourly patterns will guide data partitioning strategies\")\n",
    "print(\"   ‚Ä¢ Fare distributions inform outlier detection rules\") \n",
    "print(\"   ‚Ä¢ Correlation patterns help feature engineering\")\n",
    "print(\"   ‚Ä¢ Revenue trends support quarterly aggregation logic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a075c2d7",
   "metadata": {},
   "source": [
    "## 7. Feature Engineering\n",
    "\n",
    "Creating new features and transforming existing ones to prepare data for the batch processing pipeline and machine learning applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803369e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Engineering\n",
    "print(\"üîß FEATURE ENGINEERING\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Store original column count\n",
    "original_cols = len(df.columns)\n",
    "\n",
    "# Fix coordinate columns that might have been incorrectly converted to datetime\n",
    "coordinate_cols = ['pickup_longitude', 'pickup_latitude', 'dropoff_longitude', 'dropoff_latitude']\n",
    "for col in coordinate_cols:\n",
    "    if col in df.columns and df[col].dtype == 'datetime64[ns]':\n",
    "        print(f\"‚ö†Ô∏è Fixing incorrectly converted coordinate column: {col}\")\n",
    "        # Try to convert back to numeric, fill invalid values with NaN\n",
    "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "# 1. Time-based features\n",
    "datetime_cols = [col for col in df.columns if df[col].dtype == 'datetime64[ns]']\n",
    "pickup_col = [col for col in datetime_cols if 'pickup' in col.lower()]\n",
    "\n",
    "if pickup_col:\n",
    "    pickup_col = pickup_col[0]\n",
    "    print(f\"\\n1Ô∏è‚É£ Creating time-based features from {pickup_col}:\")\n",
    "    \n",
    "    # Extract temporal features\n",
    "    df['year'] = df[pickup_col].dt.year\n",
    "    df['month'] = df[pickup_col].dt.month\n",
    "    df['day'] = df[pickup_col].dt.day\n",
    "    df['hour'] = df[pickup_col].dt.hour\n",
    "    df['minute'] = df[pickup_col].dt.minute\n",
    "    df['day_of_week'] = df[pickup_col].dt.dayofweek  # 0=Monday, 6=Sunday\n",
    "    df['day_name'] = df[pickup_col].dt.day_name()\n",
    "    df['is_weekend'] = df['day_of_week'].isin([5, 6]).astype(int)\n",
    "    \n",
    "    # Time of day categories\n",
    "    def get_time_period(hour):\n",
    "        if 5 <= hour < 12:\n",
    "            return 'Morning'\n",
    "        elif 12 <= hour < 17:\n",
    "            return 'Afternoon'\n",
    "        elif 17 <= hour < 21:\n",
    "            return 'Evening'\n",
    "        else:\n",
    "            return 'Night'\n",
    "    \n",
    "    df['time_period'] = df['hour'].apply(get_time_period)\n",
    "    \n",
    "    # Rush hour indicator\n",
    "    df['is_rush_hour'] = ((df['hour'].isin([7, 8, 9, 17, 18, 19])) & \n",
    "                         (df['day_of_week'] < 5)).astype(int)\n",
    "    \n",
    "    print(\"   ‚úÖ Created: year, month, day, hour, minute, day_of_week, is_weekend\")\n",
    "    print(\"   ‚úÖ Created: time_period, is_rush_hour\")\n",
    "\n",
    "# 2. Distance and location features\n",
    "if 'trip_distance' in df.columns:\n",
    "    print(f\"\\n2Ô∏è‚É£ Creating distance-based features:\")\n",
    "    \n",
    "    # Distance categories\n",
    "    def categorize_distance(distance):\n",
    "        if distance <= 1:\n",
    "            return 'Short'\n",
    "        elif distance <= 5:\n",
    "            return 'Medium'\n",
    "        elif distance <= 15:\n",
    "            return 'Long'\n",
    "        else:\n",
    "            return 'Very_Long'\n",
    "    \n",
    "    df['distance_category'] = df['trip_distance'].apply(categorize_distance)\n",
    "    \n",
    "    # Distance bins for analysis\n",
    "    df['distance_bin'] = pd.cut(df['trip_distance'], \n",
    "                               bins=[0, 1, 3, 5, 10, float('inf')], \n",
    "                               labels=['0-1', '1-3', '3-5', '5-10', '10+'])\n",
    "    \n",
    "    print(\"   ‚úÖ Created: distance_category, distance_bin\")\n",
    "\n",
    "# 3. Fare and payment features\n",
    "if 'fare_amount' in df.columns:\n",
    "    print(f\"\\n3Ô∏è‚É£ Creating fare-based features:\")\n",
    "    \n",
    "    # Fare per mile (if trip distance available)\n",
    "    if 'trip_distance' in df.columns:\n",
    "        df['fare_per_mile'] = df['fare_amount'] / df['trip_distance'].replace(0, np.nan)\n",
    "        df['fare_per_mile'] = df['fare_per_mile'].fillna(df['fare_per_mile'].median())\n",
    "    \n",
    "    # Fare categories\n",
    "    def categorize_fare(fare):\n",
    "        if fare <= 10:\n",
    "            return 'Budget'\n",
    "        elif fare <= 25:\n",
    "            return 'Standard'\n",
    "        elif fare <= 50:\n",
    "            return 'Premium'\n",
    "        else:\n",
    "            return 'Luxury'\n",
    "    \n",
    "    df['fare_category'] = df['fare_amount'].apply(categorize_fare)\n",
    "    \n",
    "    # Tip features (if tip amount available)\n",
    "    if 'tip_amount' in df.columns:\n",
    "        df['tip_percentage'] = (df['tip_amount'] / df['fare_amount'] * 100).fillna(0)\n",
    "        df['is_generous_tipper'] = (df['tip_percentage'] > 20).astype(int)\n",
    "    \n",
    "    print(\"   ‚úÖ Created: fare_per_mile, fare_category\")\n",
    "    if 'tip_amount' in df.columns:\n",
    "        print(\"   ‚úÖ Created: tip_percentage, is_generous_tipper\")\n",
    "\n",
    "# 4. Trip duration features (if available)\n",
    "if 'trip_duration_minutes' in df.columns:\n",
    "    print(f\"\\n4Ô∏è‚É£ Creating duration-based features:\")\n",
    "    \n",
    "    # Speed calculation\n",
    "    if 'trip_distance' in df.columns:\n",
    "        df['average_speed_mph'] = (df['trip_distance'] / (df['trip_duration_minutes'] / 60)).replace([np.inf, -np.inf], np.nan)\n",
    "        df['average_speed_mph'] = df['average_speed_mph'].fillna(df['average_speed_mph'].median())\n",
    "    \n",
    "    # Duration categories\n",
    "    def categorize_duration(duration):\n",
    "        if duration <= 10:\n",
    "            return 'Quick'\n",
    "        elif duration <= 30:\n",
    "            return 'Normal'\n",
    "        elif duration <= 60:\n",
    "            return 'Long'\n",
    "        else:\n",
    "            return 'Very_Long'\n",
    "    \n",
    "    df['duration_category'] = df['trip_duration_minutes'].apply(categorize_duration)\n",
    "    \n",
    "    print(\"   ‚úÖ Created: average_speed_mph, duration_category\")\n",
    "\n",
    "# 5. Passenger and capacity features\n",
    "if 'passenger_count' in df.columns:\n",
    "    print(f\"\\n5Ô∏è‚É£ Creating passenger-based features:\")\n",
    "    \n",
    "    # Group size categories\n",
    "    def categorize_group_size(passengers):\n",
    "        if passengers == 1:\n",
    "            return 'Solo'\n",
    "        elif passengers == 2:\n",
    "            return 'Couple'\n",
    "        elif passengers <= 4:\n",
    "            return 'Small_Group'\n",
    "        else:\n",
    "            return 'Large_Group'\n",
    "    \n",
    "    df['group_size_category'] = df['passenger_count'].apply(categorize_group_size)\n",
    "    \n",
    "    # Revenue per passenger\n",
    "    if 'fare_amount' in df.columns:\n",
    "        df['fare_per_passenger'] = df['fare_amount'] / df['passenger_count']\n",
    "    \n",
    "    print(\"   ‚úÖ Created: group_size_category, fare_per_passenger\")\n",
    "\n",
    "# 6. Location-based features (if coordinates available and properly formatted)\n",
    "location_cols = [col for col in df.columns if 'longitude' in col.lower() or 'latitude' in col.lower()]\n",
    "numeric_location_cols = [col for col in location_cols if pd.api.types.is_numeric_dtype(df[col])]\n",
    "\n",
    "if len(numeric_location_cols) >= 4:  # pickup and dropoff coordinates\n",
    "    print(f\"\\n6Ô∏è‚É£ Creating location-based features:\")\n",
    "    \n",
    "    # Find coordinate columns\n",
    "    pickup_lat_col = [col for col in numeric_location_cols if 'pickup' in col.lower() and 'lat' in col.lower()]\n",
    "    pickup_lon_col = [col for col in numeric_location_cols if 'pickup' in col.lower() and 'lon' in col.lower()]\n",
    "    dropoff_lat_col = [col for col in numeric_location_cols if 'dropoff' in col.lower() and 'lat' in col.lower()]\n",
    "    dropoff_lon_col = [col for col in numeric_location_cols if 'dropoff' in col.lower() and 'lon' in col.lower()]\n",
    "    \n",
    "    if pickup_lat_col and pickup_lon_col and dropoff_lat_col and dropoff_lon_col:\n",
    "        try:\n",
    "            # Simplified distance calculation (not exact but good for features)\n",
    "            lat_diff = df[dropoff_lat_col[0]] - df[pickup_lat_col[0]]\n",
    "            lon_diff = df[dropoff_lon_col[0]] - df[pickup_lon_col[0]]\n",
    "            df['straight_line_distance'] = np.sqrt(lat_diff**2 + lon_diff**2) * 111  # Approximate km\n",
    "            \n",
    "            print(\"   ‚úÖ Created: straight_line_distance\")\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è Could not create location features: {e}\")\n",
    "            print(f\"   üìä Coordinate column types: {[(col, df[col].dtype) for col in [pickup_lat_col[0], pickup_lon_col[0], dropoff_lat_col[0], dropoff_lon_col[0]]]}\")\n",
    "else:\n",
    "    print(f\"\\n6Ô∏è‚É£ Skipping location-based features:\")\n",
    "    print(f\"   ‚ö†Ô∏è Insufficient numeric coordinate columns found: {len(numeric_location_cols)}/4 needed\")\n",
    "    if location_cols:\n",
    "        print(f\"   üìä Available location columns: {[(col, df[col].dtype) for col in location_cols]}\")\n",
    "\n",
    "# 7. Quarterly aggregation features (for batch processing)\n",
    "if pickup_col:\n",
    "    print(f\"\\n7Ô∏è‚É£ Creating quarterly aggregation features:\")\n",
    "    \n",
    "    # Quarter identification\n",
    "    df['quarter'] = df[pickup_col].dt.quarter\n",
    "    df['year_quarter'] = df['year'].astype(str) + '_Q' + df['quarter'].astype(str)\n",
    "    \n",
    "    # Monthly aggregation\n",
    "    df['year_month'] = df[pickup_col].dt.to_period('M').astype(str)\n",
    "    \n",
    "    print(\"   ‚úÖ Created: quarter, year_quarter, year_month\")\n",
    "\n",
    "# 8. One-hot encoding for categorical features\n",
    "print(f\"\\n8Ô∏è‚É£ One-hot encoding categorical features:\")\n",
    "\n",
    "categorical_features = ['time_period', 'distance_category', 'fare_category', 'duration_category', 'group_size_category']\n",
    "existing_categorical = [col for col in categorical_features if col in df.columns]\n",
    "\n",
    "if existing_categorical:\n",
    "    # Create dummy variables\n",
    "    df_encoded = pd.get_dummies(df, columns=existing_categorical, prefix=existing_categorical, drop_first=True)\n",
    "    \n",
    "    # Update dataframe\n",
    "    new_dummy_cols = [col for col in df_encoded.columns if col not in df.columns]\n",
    "    for col in new_dummy_cols:\n",
    "        df[col] = df_encoded[col]\n",
    "    \n",
    "    print(f\"   ‚úÖ Created {len(new_dummy_cols)} dummy variables from {len(existing_categorical)} categorical features\")\n",
    "\n",
    "# 9. Feature summary\n",
    "new_cols = len(df.columns)\n",
    "print(f\"\\n‚úÖ FEATURE ENGINEERING COMPLETED!\")\n",
    "print(f\"üìä Original columns: {original_cols}\")\n",
    "print(f\"üìä New columns: {new_cols}\")\n",
    "print(f\"üìä Features added: {new_cols - original_cols}\")\n",
    "\n",
    "# Display new feature summary\n",
    "print(f\"\\nüìã New Feature Categories:\")\n",
    "time_features = [col for col in df.columns if col in ['year', 'month', 'day', 'hour', 'is_weekend', 'is_rush_hour', 'time_period']]\n",
    "distance_features = [col for col in df.columns if 'distance' in col.lower() or 'speed' in col.lower()]\n",
    "fare_features = [col for col in df.columns if 'fare' in col.lower() or 'tip' in col.lower()]\n",
    "categorical_features = [col for col in df.columns if col.endswith(('_Budget', '_Standard', '_Premium', '_Solo', '_Couple', '_Morning', '_Afternoon', '_Evening', '_Night'))]\n",
    "\n",
    "print(f\"   ‚è∞ Time features ({len(time_features)}): {time_features[:5]}...\")\n",
    "print(f\"   üõ£Ô∏è Distance features ({len(distance_features)}): {distance_features}\")\n",
    "print(f\"   üí∞ Fare features ({len(fare_features)}): {fare_features}\")\n",
    "print(f\"   üè∑Ô∏è Encoded features ({len(categorical_features)}): {categorical_features[:5]}...\")\n",
    "\n",
    "print(f\"\\nüéØ Dataset ready for batch processing and ML model training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830b79d1",
   "metadata": {},
   "source": [
    "## 8. Export Processed Data\n",
    "\n",
    "Saving the cleaned and processed dataset to the Kaggle working directory for use in the batch processing pipeline and ML model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bce941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export processed data to local storage and AWS S3\n",
    "print(\"üíæ EXPORTING PROCESSED DATA TO LOCAL AND AWS S3\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Import AWS SDK\n",
    "try:\n",
    "    import boto3\n",
    "    from botocore.exceptions import ClientError, NoCredentialsError\n",
    "    s3_available = True\n",
    "    print(\"‚úÖ AWS SDK (boto3) imported successfully\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è Installing boto3 for AWS S3 integration...\")\n",
    "    import subprocess\n",
    "    import sys\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"boto3\"])\n",
    "    import boto3\n",
    "    from botocore.exceptions import ClientError, NoCredentialsError\n",
    "    s3_available = True\n",
    "    print(\"‚úÖ boto3 installed and imported successfully\")\n",
    "\n",
    "# AWS Configuration (use environment variables for credentials)\n",
    "import os\n",
    "AWS_ACCESS_KEY_ID = os.getenv('AWS_ACCESS_KEY_ID', '')\n",
    "AWS_SECRET_ACCESS_KEY = os.getenv('AWS_SECRET_ACCESS_KEY', '')\n",
    "AWS_REGION = os.getenv('AWS_REGION', 'us-east-1')\n",
    "S3_BUCKET_NAME = 'nyc-taxi-batch-processing'  # You may need to change this to an existing bucket\n",
    "S3_PREFIX = f'taxi-data/processed/{datetime.now().strftime(\"%Y/%m/%d\")}'\n",
    "\n",
    "# Initialize S3 client\n",
    "try:\n",
    "    s3_client = boto3.client(\n",
    "        's3',\n",
    "        aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
    "        aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n",
    "        region_name=AWS_REGION\n",
    "    )\n",
    "    print(f\"‚úÖ AWS S3 client initialized for region: {AWS_REGION}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Failed to initialize S3 client: {e}\")\n",
    "    s3_available = False\n",
    "\n",
    "# Function to upload file to S3\n",
    "def upload_to_s3(local_file_path, s3_key, description=\"file\"):\n",
    "    \"\"\"Upload a file to S3 bucket\"\"\"\n",
    "    try:\n",
    "        file_size = os.path.getsize(local_file_path) / (1024*1024)\n",
    "        print(f\"   üì§ Uploading {description} to S3: s3://{S3_BUCKET_NAME}/{s3_key}\")\n",
    "        \n",
    "        s3_client.upload_file(local_file_path, S3_BUCKET_NAME, s3_key)\n",
    "        print(f\"   ‚úÖ Successfully uploaded {description} ({file_size:.2f} MB)\")\n",
    "        return True\n",
    "    except FileNotFoundError:\n",
    "        print(f\"   ‚ùå Local file not found: {local_file_path}\")\n",
    "        return False\n",
    "    except NoCredentialsError:\n",
    "        print(f\"   ‚ùå AWS credentials not found or invalid\")\n",
    "        return False\n",
    "    except ClientError as e:\n",
    "        print(f\"   ‚ùå AWS S3 error: {e}\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Unexpected error uploading {description}: {e}\")\n",
    "        return False\n",
    "\n",
    "# 1. Create export directory structure\n",
    "export_base = '/kaggle/working/'\n",
    "export_dirs = ['processed_data', 'quarterly_aggregations', 'ml_features', 'emr_ready']\n",
    "\n",
    "for directory in export_dirs:\n",
    "    dir_path = os.path.join(export_base, directory)\n",
    "    os.makedirs(dir_path, exist_ok=True)\n",
    "    print(f\"üìÅ Created directory: {dir_path}\")\n",
    "\n",
    "# 2. Save main processed dataset\n",
    "print(f\"\\n1Ô∏è‚É£ Saving main processed dataset:\")\n",
    "\n",
    "# Full dataset in CSV format\n",
    "main_output_path = os.path.join(export_base, 'processed_data', 'nyc_taxi_processed.csv')\n",
    "df.to_csv(main_output_path, index=False)\n",
    "print(f\"   ‚úÖ Saved full dataset: {main_output_path}\")\n",
    "print(f\"      Rows: {len(df):,}, Columns: {len(df.columns)}\")\n",
    "print(f\"      Size: {os.path.getsize(main_output_path) / (1024*1024):.2f} MB\")\n",
    "\n",
    "# Parquet format for better performance in Spark/EMR\n",
    "parquet_output_path = os.path.join(export_base, 'processed_data', 'nyc_taxi_processed.parquet')\n",
    "df.to_parquet(parquet_output_path, index=False)\n",
    "print(f\"   ‚úÖ Saved parquet format: {parquet_output_path}\")\n",
    "print(f\"      Size: {os.path.getsize(parquet_output_path) / (1024*1024):.2f} MB\")\n",
    "\n",
    "# Upload to S3\n",
    "if s3_available:\n",
    "    print(f\"\\n   üåê Uploading main dataset to AWS S3:\")\n",
    "    upload_to_s3(main_output_path, f\"{S3_PREFIX}/processed_data/nyc_taxi_processed.csv\", \"main dataset (CSV)\")\n",
    "    upload_to_s3(parquet_output_path, f\"{S3_PREFIX}/processed_data/nyc_taxi_processed.parquet\", \"main dataset (Parquet)\")\n",
    "\n",
    "# 3. Create quarterly aggregations (for batch processing simulation)\n",
    "if 'year_quarter' in df.columns:\n",
    "    print(f\"\\n2Ô∏è‚É£ Creating quarterly aggregations:\")\n",
    "    \n",
    "    # Quarterly summary statistics\n",
    "    quarterly_agg = df.groupby('year_quarter').agg({\n",
    "        'fare_amount': ['count', 'sum', 'mean', 'std'] if 'fare_amount' in df.columns else 'count',\n",
    "        'trip_distance': ['mean', 'sum'] if 'trip_distance' in df.columns else 'count',\n",
    "        'passenger_count': ['sum', 'mean'] if 'passenger_count' in df.columns else 'count',\n",
    "        'tip_amount': ['sum', 'mean'] if 'tip_amount' in df.columns else 'count'\n",
    "    })\n",
    "    \n",
    "    # Flatten column names\n",
    "    quarterly_agg.columns = [f\"{col[0]}_{col[1]}\" if isinstance(col, tuple) else col for col in quarterly_agg.columns]\n",
    "    quarterly_agg = quarterly_agg.reset_index()\n",
    "    \n",
    "    # Save quarterly aggregations\n",
    "    quarterly_output_path = os.path.join(export_base, 'quarterly_aggregations', 'quarterly_summary.csv')\n",
    "    quarterly_agg.to_csv(quarterly_output_path, index=False)\n",
    "    print(f\"   ‚úÖ Saved quarterly aggregations: {quarterly_output_path}\")\n",
    "    print(f\"      Quarters: {len(quarterly_agg)}\")\n",
    "    \n",
    "    # Upload quarterly data to S3\n",
    "    if s3_available:\n",
    "        upload_to_s3(quarterly_output_path, f\"{S3_PREFIX}/quarterly_aggregations/quarterly_summary.csv\", \"quarterly aggregations\")\n",
    "    \n",
    "    # Display quarterly summary\n",
    "    print(f\"   üìä Quarterly Summary:\")\n",
    "    for _, row in quarterly_agg.head().iterrows():\n",
    "        quarter = row['year_quarter']\n",
    "        if 'fare_amount_count' in quarterly_agg.columns:\n",
    "            trips = int(row['fare_amount_count'])\n",
    "            revenue = row['fare_amount_sum'] if 'fare_amount_sum' in quarterly_agg.columns else 0\n",
    "            print(f\"      {quarter}: {trips:,} trips, ${revenue:,.2f} revenue\")\n",
    "\n",
    "# 4. Create ML-ready feature sets for EMR\n",
    "print(f\"\\n3Ô∏è‚É£ Creating ML-ready feature sets for AWS EMR:\")\n",
    "\n",
    "# Numerical features only\n",
    "numerical_features = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "ml_numerical = df[numerical_features].copy()\n",
    "\n",
    "ml_numerical_path = os.path.join(export_base, 'ml_features', 'numerical_features.csv')\n",
    "ml_numerical.to_csv(ml_numerical_path, index=False)\n",
    "print(f\"   ‚úÖ Saved numerical features: {ml_numerical_path}\")\n",
    "print(f\"      Features: {len(numerical_features)}\")\n",
    "\n",
    "# Target variables for different ML tasks\n",
    "if 'fare_amount' in df.columns:\n",
    "    # Fare prediction dataset\n",
    "    fare_prediction_features = ['trip_distance', 'passenger_count', 'hour', 'day_of_week', 'is_weekend', 'is_rush_hour']\n",
    "    fare_prediction_features = [col for col in fare_prediction_features if col in df.columns]\n",
    "    \n",
    "    if fare_prediction_features:\n",
    "        fare_ml_data = df[fare_prediction_features + ['fare_amount']].copy()\n",
    "        fare_ml_path = os.path.join(export_base, 'ml_features', 'fare_prediction_data.csv')\n",
    "        fare_ml_data.to_csv(fare_ml_path, index=False)\n",
    "        print(f\"   ‚úÖ Saved fare prediction dataset: {fare_ml_path}\")\n",
    "        print(f\"      Features for fare prediction: {len(fare_prediction_features)}\")\n",
    "        \n",
    "        # Create EMR-optimized version (smaller chunks for distributed processing)\n",
    "        chunk_size = 10000\n",
    "        emr_dir = os.path.join(export_base, 'emr_ready', 'fare_prediction_chunks')\n",
    "        os.makedirs(emr_dir, exist_ok=True)\n",
    "        \n",
    "        for i, chunk in enumerate(range(0, len(fare_ml_data), chunk_size)):\n",
    "            chunk_data = fare_ml_data.iloc[chunk:chunk + chunk_size]\n",
    "            chunk_path = os.path.join(emr_dir, f'fare_prediction_chunk_{i:03d}.csv')\n",
    "            chunk_data.to_csv(chunk_path, index=False)\n",
    "            \n",
    "            # Upload chunk to S3\n",
    "            if s3_available:\n",
    "                upload_to_s3(chunk_path, f\"{S3_PREFIX}/emr_ready/fare_prediction_chunks/fare_prediction_chunk_{i:03d}.csv\", f\"fare prediction chunk {i}\")\n",
    "        \n",
    "        print(f\"   ‚úÖ Created {i+1} chunks for EMR distributed processing\")\n",
    "\n",
    "# Upload ML features to S3\n",
    "if s3_available:\n",
    "    print(f\"\\n   üåê Uploading ML features to AWS S3:\")\n",
    "    upload_to_s3(ml_numerical_path, f\"{S3_PREFIX}/ml_features/numerical_features.csv\", \"numerical features\")\n",
    "    if 'fare_amount' in df.columns:\n",
    "        upload_to_s3(fare_ml_path, f\"{S3_PREFIX}/ml_features/fare_prediction_data.csv\", \"fare prediction dataset\")\n",
    "\n",
    "# 5. Create EMR job configuration files\n",
    "print(f\"\\n4Ô∏è‚É£ Creating AWS EMR job configuration files:\")\n",
    "\n",
    "# EMR Spark job configuration for ML training\n",
    "emr_spark_config = {\n",
    "    \"Name\": \"NYC-Taxi-ML-Training\",\n",
    "    \"ReleaseLabel\": \"emr-6.4.0\",\n",
    "    \"Applications\": [\n",
    "        {\"Name\": \"Spark\"},\n",
    "        {\"Name\": \"Hadoop\"}\n",
    "    ],\n",
    "    \"Instances\": {\n",
    "        \"InstanceGroups\": [\n",
    "            {\n",
    "                \"Name\": \"Master\",\n",
    "                \"Market\": \"ON_DEMAND\",\n",
    "                \"InstanceRole\": \"MASTER\",\n",
    "                \"InstanceType\": \"m5.xlarge\",\n",
    "                \"InstanceCount\": 1\n",
    "            },\n",
    "            {\n",
    "                \"Name\": \"Workers\",\n",
    "                \"Market\": \"ON_DEMAND\", \n",
    "                \"InstanceRole\": \"CORE\",\n",
    "                \"InstanceType\": \"m5.xlarge\",\n",
    "                \"InstanceCount\": 2\n",
    "            }\n",
    "        ]\n",
    "    },\n",
    "    \"Steps\": [\n",
    "        {\n",
    "            \"Name\": \"Fare Prediction Model Training\",\n",
    "            \"ActionOnFailure\": \"TERMINATE_CLUSTER\",\n",
    "            \"HadoopJarStep\": {\n",
    "                \"Jar\": \"command-runner.jar\",\n",
    "                \"Args\": [\n",
    "                    \"spark-submit\",\n",
    "                    \"--deploy-mode\", \"cluster\",\n",
    "                    \"--class\", \"org.apache.spark.examples.ml.LinearRegressionExample\",\n",
    "                    f\"s3://{S3_BUCKET_NAME}/{S3_PREFIX}/emr_ready/fare_prediction_chunks/\"\n",
    "                ]\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    "    \"ServiceRole\": \"EMR_DefaultRole\",\n",
    "    \"JobFlowRole\": \"EMR_EC2_DefaultRole\"\n",
    "}\n",
    "\n",
    "# Save EMR configuration\n",
    "emr_config_path = os.path.join(export_base, 'emr_ready', 'emr_cluster_config.json')\n",
    "import json\n",
    "with open(emr_config_path, 'w') as f:\n",
    "    json.dump(emr_spark_config, f, indent=2)\n",
    "print(f\"   ‚úÖ Saved EMR cluster configuration: {emr_config_path}\")\n",
    "\n",
    "# Create PySpark ML training script\n",
    "pyspark_script = '''\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"NYC-Taxi-Fare-Prediction\").getOrCreate()\n",
    "\n",
    "# Load data from S3\n",
    "df = spark.read.option(\"header\", \"true\").option(\"inferSchema\", \"true\").csv(\"s3://''' + S3_BUCKET_NAME + '/' + S3_PREFIX + '''/emr_ready/fare_prediction_chunks/*.csv\")\n",
    "\n",
    "# Prepare features\n",
    "feature_cols = ['trip_distance', 'passenger_count', 'hour', 'day_of_week', 'is_weekend', 'is_rush_hour']\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "df_assembled = assembler.transform(df)\n",
    "\n",
    "# Split data\n",
    "train_data, test_data = df_assembled.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Train model\n",
    "lr = LinearRegression(featuresCol=\"features\", labelCol=\"fare_amount\")\n",
    "model = lr.fit(train_data)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.transform(test_data)\n",
    "\n",
    "# Evaluate model\n",
    "evaluator = RegressionEvaluator(labelCol=\"fare_amount\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmse = evaluator.evaluate(predictions)\n",
    "\n",
    "print(f\"Root Mean Squared Error: {rmse}\")\n",
    "\n",
    "# Save model to S3\n",
    "model.write().overwrite().save(\"s3://''' + S3_BUCKET_NAME + '/' + S3_PREFIX + '''/models/fare_prediction_model\")\n",
    "\n",
    "spark.stop()\n",
    "'''\n",
    "\n",
    "pyspark_script_path = os.path.join(export_base, 'emr_ready', 'fare_prediction_training.py')\n",
    "with open(pyspark_script_path, 'w') as f:\n",
    "    f.write(pyspark_script)\n",
    "print(f\"   ‚úÖ Saved PySpark ML training script: {pyspark_script_path}\")\n",
    "\n",
    "# Upload EMR files to S3\n",
    "if s3_available:\n",
    "    print(f\"\\n   üåê Uploading EMR configuration files to S3:\")\n",
    "    upload_to_s3(emr_config_path, f\"{S3_PREFIX}/emr_ready/emr_cluster_config.json\", \"EMR cluster configuration\")\n",
    "    upload_to_s3(pyspark_script_path, f\"{S3_PREFIX}/emr_ready/fare_prediction_training.py\", \"PySpark ML training script\")\n",
    "\n",
    "# 6. Create data dictionary/schema\n",
    "print(f\"\\n5Ô∏è‚É£ Creating data dictionary:\")\n",
    "\n",
    "# Generate schema information\n",
    "schema_info = []\n",
    "for col in df.columns:\n",
    "    col_info = {\n",
    "        'column_name': col,\n",
    "        'data_type': str(df[col].dtype),\n",
    "        'non_null_count': df[col].count(),\n",
    "        'null_count': df[col].isnull().sum(),\n",
    "        'null_percentage': round(df[col].isnull().sum() / len(df) * 100, 2),\n",
    "        'unique_values': df[col].nunique(),\n",
    "        'example_values': str(df[col].dropna().head(3).tolist()) if df[col].count() > 0 else 'No data'\n",
    "    }\n",
    "    schema_info.append(col_info)\n",
    "\n",
    "schema_df = pd.DataFrame(schema_info)\n",
    "schema_path = os.path.join(export_base, 'processed_data', 'data_schema.csv')\n",
    "schema_df.to_csv(schema_path, index=False)\n",
    "print(f\"   ‚úÖ Saved data schema: {schema_path}\")\n",
    "\n",
    "# Upload schema to S3\n",
    "if s3_available:\n",
    "    upload_to_s3(schema_path, f\"{S3_PREFIX}/processed_data/data_schema.csv\", \"data schema\")\n",
    "\n",
    "# 7. Create processing summary report\n",
    "print(f\"\\n6Ô∏è‚É£ Creating processing summary report:\")\n",
    "\n",
    "processing_summary = {\n",
    "    'processing_timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'original_rows': original_shape[0] if 'original_shape' in locals() else len(df),\n",
    "    'processed_rows': len(df),\n",
    "    'rows_removed': (original_shape[0] - len(df)) if 'original_shape' in locals() else 0,\n",
    "    'original_columns': original_cols if 'original_cols' in locals() else len(df.columns),\n",
    "    'final_columns': len(df.columns),\n",
    "    'features_created': len(df.columns) - (original_cols if 'original_cols' in locals() else len(df.columns)),\n",
    "    'data_quality_score': round((1 - df.isnull().sum().sum() / (len(df) * len(df.columns))) * 100, 2),\n",
    "    'dataset_size_mb': round(df.memory_usage(deep=True).sum() / (1024*1024), 2),\n",
    "    'ready_for_ml': True,\n",
    "    's3_bucket': S3_BUCKET_NAME,\n",
    "    's3_prefix': S3_PREFIX,\n",
    "    'emr_ready': True\n",
    "}\n",
    "\n",
    "# Save processing summary\n",
    "summary_df = pd.DataFrame([processing_summary])\n",
    "summary_path = os.path.join(export_base, 'processed_data', 'processing_summary.csv')\n",
    "summary_df.to_csv(summary_path, index=False)\n",
    "print(f\"   ‚úÖ Saved processing summary: {summary_path}\")\n",
    "\n",
    "# Upload summary to S3\n",
    "if s3_available:\n",
    "    upload_to_s3(summary_path, f\"{S3_PREFIX}/processed_data/processing_summary.csv\", \"processing summary\")\n",
    "\n",
    "# Display summary\n",
    "print(f\"\\nüìä PROCESSING SUMMARY:\")\n",
    "print(f\"   üïê Processed at: {processing_summary['processing_timestamp']}\")\n",
    "print(f\"   üìà Rows: {processing_summary['original_rows']:,} ‚Üí {processing_summary['processed_rows']:,}\")\n",
    "print(f\"   üìä Columns: {processing_summary['original_columns']} ‚Üí {processing_summary['final_columns']}\")\n",
    "print(f\"   üîß Features created: {processing_summary['features_created']}\")\n",
    "print(f\"   ‚ú® Data quality score: {processing_summary['data_quality_score']}%\")\n",
    "print(f\"   üíæ Dataset size: {processing_summary['dataset_size_mb']} MB\")\n",
    "print(f\"   üåê S3 Location: s3://{S3_BUCKET_NAME}/{S3_PREFIX}/\")\n",
    "\n",
    "# 8. List all exported files (local and S3)\n",
    "print(f\"\\nüìÅ EXPORTED FILES (LOCAL):\")\n",
    "for root, dirs, files in os.walk(export_base):\n",
    "    level = root.replace(export_base, '').count(os.sep)\n",
    "    indent = ' ' * 2 * level\n",
    "    print(f\"{indent}üìÅ {os.path.basename(root)}/\")\n",
    "    sub_indent = ' ' * 2 * (level + 1)\n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "        file_size = os.path.getsize(file_path) / (1024*1024)\n",
    "        print(f\"{sub_indent}üìÑ {file} ({file_size:.2f} MB)\")\n",
    "\n",
    "# 9. EMR Usage Instructions\n",
    "print(f\"\\nüöÄ AWS EMR USAGE INSTRUCTIONS:\")\n",
    "print(f\"=\" * 50)\n",
    "print(f\"üìã To run ML training on AWS EMR:\")\n",
    "print(f\"\")\n",
    "print(f\"1Ô∏è‚É£ Create EMR Cluster:\")\n",
    "print(f\"   aws emr create-cluster --cli-input-json file://emr_cluster_config.json\")\n",
    "print(f\"\")\n",
    "print(f\"2Ô∏è‚É£ Add training step to existing cluster:\")\n",
    "print(f\"   aws emr add-steps --cluster-id <cluster-id> --steps file://training_step.json\")\n",
    "print(f\"\")\n",
    "print(f\"3Ô∏è‚É£ Monitor cluster:\")\n",
    "print(f\"   aws emr describe-cluster --cluster-id <cluster-id>\")\n",
    "print(f\"\")\n",
    "print(f\"4Ô∏è‚É£ Access S3 data:\")\n",
    "print(f\"   üìÇ Main dataset: s3://{S3_BUCKET_NAME}/{S3_PREFIX}/processed_data/\")\n",
    "print(f\"   ü§ñ ML features: s3://{S3_BUCKET_NAME}/{S3_PREFIX}/ml_features/\")\n",
    "print(f\"   ‚ö° EMR scripts: s3://{S3_BUCKET_NAME}/{S3_PREFIX}/emr_ready/\")\n",
    "print(f\"\")\n",
    "print(f\"5Ô∏è‚É£ Expected outputs:\")\n",
    "print(f\"   üìà Trained models: s3://{S3_BUCKET_NAME}/{S3_PREFIX}/models/\")\n",
    "print(f\"   üìä Evaluation metrics: EMR step logs\")\n",
    "\n",
    "print(f\"\\n‚úÖ DATA EXPORT TO AWS S3 COMPLETED!\")\n",
    "print(f\"üéØ All files uploaded to S3 and ready for AWS EMR MapReduce ML training!\")\n",
    "print(f\"üöÄ Next steps: Launch EMR cluster and run the fare prediction training job\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bf84f8",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "### üéØ Batch Processing Pipeline Implementation\n",
    "\n",
    "This notebook has successfully demonstrated the data preparation and analysis pipeline for the NYC Yellow Taxi dataset, which forms the foundation of our batch processing data architecture.\n",
    "\n",
    "### ‚úÖ What We've Accomplished:\n",
    "\n",
    "1. **Environment Setup**: Configured Kaggle environment with essential data science libraries\n",
    "2. **Data Exploration**: Identified and loaded NYC taxi dataset with comprehensive inspection\n",
    "3. **Data Cleaning**: Handled missing values, removed duplicates, and filtered invalid records\n",
    "4. **Exploratory Analysis**: Analyzed distributions, correlations, and business patterns\n",
    "5. **Visualization**: Created comprehensive charts showing trip patterns, revenue trends, and correlations\n",
    "6. **Feature Engineering**: Built 25+ new features for time, distance, fare, and categorical analysis\n",
    "7. **Data Export**: Saved processed datasets in multiple formats for batch processing pipeline\n",
    "\n",
    "### üèóÔ∏è Integration with Batch Processing Architecture:\n",
    "\n",
    "This notebook represents the **Data Processing Service** component of our microservices architecture:\n",
    "\n",
    "- **Input**: Raw NYC taxi data from Kaggle/file ingestion\n",
    "- **Processing**: ETL operations, cleaning, feature engineering, and aggregations\n",
    "- **Output**: ML-ready datasets for quarterly model training\n",
    "\n",
    "### üìä Key Insights for Quarterly ML Models:\n",
    "\n",
    "- **Fare Prediction**: Average fare $12-15, strong correlation with distance and time\n",
    "- **Demand Forecasting**: Peak hours 7-9 AM and 5-7 PM, weekend patterns differ significantly\n",
    "- **Route Optimization**: Trip distances follow exponential distribution, most trips under 5 miles\n",
    "- **Revenue Analytics**: Clear seasonal and daily patterns suitable for quarterly aggregation\n",
    "\n",
    "### üöÄ Next Steps for Full Implementation:\n",
    "\n",
    "1. **Deploy to Spark Cluster**: Run this processing logic on Apache Spark for scalability\n",
    "2. **Integrate with Airflow**: Schedule quarterly processing via Airflow DAGs\n",
    "3. **Store in HDFS**: Save processed data to Hadoop HDFS with proper partitioning\n",
    "4. **ML Model Training**: Use exported features for fare prediction and demand forecasting models\n",
    "5. **API Delivery**: Serve aggregated data through FastAPI endpoints\n",
    "\n",
    "### üéì Data Engineering Skills Demonstrated:\n",
    "\n",
    "- **Batch Processing**: Large-scale data preparation and aggregation\n",
    "- **ETL Pipeline**: Extract, Transform, Load operations with quality checks\n",
    "- **Feature Engineering**: Creating ML-ready features from raw transactional data\n",
    "- **Data Quality**: Comprehensive cleaning and validation processes\n",
    "- **Scalable Design**: Code structure suitable for distributed processing\n",
    "\n",
    "This implementation provides a solid foundation for the complete batch processing data architecture required for the portfolio project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5b50b3",
   "metadata": {},
   "source": [
    "## AWS EMR Setup and Deployment Commands\n",
    "\n",
    "### üöÄ Quick Start Commands for AWS EMR\n",
    "\n",
    "Run these commands in your local terminal (with AWS CLI configured) to launch the ML training pipeline on AWS EMR:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025f590f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AWS EMR Deployment Commands\n",
    "print(\"üöÄ AWS EMR DEPLOYMENT COMMANDS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Generate AWS CLI commands for easy copy-paste\n",
    "S3_BUCKET_NAME = 'nyc-taxi-batch-processing'  # Update this to your actual bucket name\n",
    "S3_PREFIX = f'taxi-data/processed/{datetime.now().strftime(\"%Y/%m/%d\")}'\n",
    "\n",
    "print(f\"üìã Copy these commands to deploy your ML pipeline to AWS EMR:\")\n",
    "print(f\"\")\n",
    "\n",
    "# 1. Create S3 bucket (if needed)\n",
    "print(f\"1Ô∏è‚É£ Create S3 bucket (if it doesn't exist):\")\n",
    "print(f\"aws s3 mb s3://{S3_BUCKET_NAME}\")\n",
    "print(f\"\")\n",
    "\n",
    "# 2. Verify data upload\n",
    "print(f\"2Ô∏è‚É£ Verify data was uploaded to S3:\")\n",
    "print(f\"aws s3 ls s3://{S3_BUCKET_NAME}/{S3_PREFIX}/ --recursive\")\n",
    "print(f\"\")\n",
    "\n",
    "# 3. Create EMR cluster\n",
    "print(f\"3Ô∏è‚É£ Create EMR cluster for ML training:\")\n",
    "emr_create_command = f'''aws emr create-cluster \\\\\n",
    "  --name \"NYC-Taxi-ML-Training-{datetime.now().strftime('%Y%m%d-%H%M')}\" \\\\\n",
    "  --release-label emr-6.4.0 \\\\\n",
    "  --applications Name=Spark Name=Hadoop \\\\\n",
    "  --instance-type m5.xlarge \\\\\n",
    "  --instance-count 3 \\\\\n",
    "  --service-role EMR_DefaultRole \\\\\n",
    "  --ec2-attributes InstanceProfile=EMR_EC2_DefaultRole \\\\\n",
    "  --bootstrap-actions Path=s3://elasticmapreduce/bootstrap-actions/run-if,Args=[\"instance.isMaster=true\",\"echo 'Master node setup'\"] \\\\\n",
    "  --log-uri s3://{S3_BUCKET_NAME}/emr-logs/ \\\\\n",
    "  --auto-terminate'''\n",
    "\n",
    "print(emr_create_command)\n",
    "print(f\"\")\n",
    "\n",
    "# 4. Add training step\n",
    "print(f\"4Ô∏è‚É£ Add ML training step to cluster (replace <cluster-id> with actual cluster ID):\")\n",
    "training_step_command = f'''aws emr add-steps --cluster-id <cluster-id> --steps '[{{\n",
    "  \"Name\": \"Fare-Prediction-Training\",\n",
    "  \"ActionOnFailure\": \"CONTINUE\",\n",
    "  \"HadoopJarStep\": {{\n",
    "    \"Jar\": \"command-runner.jar\",\n",
    "    \"Args\": [\n",
    "      \"spark-submit\",\n",
    "      \"--deploy-mode\", \"cluster\", \n",
    "      \"--executor-memory\", \"2g\",\n",
    "      \"--driver-memory\", \"1g\",\n",
    "      \"s3://{S3_BUCKET_NAME}/{S3_PREFIX}/emr_ready/fare_prediction_training.py\"\n",
    "    ]\n",
    "  }}\n",
    "}}]\\'\n",
    "'''\n",
    "\n",
    "print(training_step_command)\n",
    "print(f\"\")\n",
    "\n",
    "# 5. Monitor cluster\n",
    "print(f\"5Ô∏è‚É£ Monitor cluster status:\")\n",
    "print(f\"aws emr describe-cluster --cluster-id <cluster-id>\")\n",
    "print(f\"aws emr list-steps --cluster-id <cluster-id>\")\n",
    "print(f\"\")\n",
    "\n",
    "# 6. Download results\n",
    "print(f\"6Ô∏è‚É£ Download trained models and results:\")\n",
    "print(f\"aws s3 sync s3://{S3_BUCKET_NAME}/{S3_PREFIX}/models/ ./trained_models/\")\n",
    "print(f\"aws s3 cp s3://{S3_BUCKET_NAME}/emr-logs/ ./emr_logs/ --recursive\")\n",
    "print(f\"\")\n",
    "\n",
    "# 7. Cost optimization\n",
    "print(f\"7Ô∏è‚É£ Terminate cluster when done (to save costs):\")\n",
    "print(f\"aws emr terminate-clusters --cluster-ids <cluster-id>\")\n",
    "print(f\"\")\n",
    "\n",
    "print(f\"üí° TIPS:\")\n",
    "print(f\"   ‚Ä¢ Replace <cluster-id> with the actual cluster ID from step 3\")\n",
    "print(f\"   ‚Ä¢ Monitor costs in AWS Console ‚Üí Billing\")\n",
    "print(f\"   ‚Ä¢ Use Spot instances for cost savings: --bid-price 0.05\")\n",
    "print(f\"   ‚Ä¢ Check EMR logs for debugging: s3://{S3_BUCKET_NAME}/emr-logs/\")\n",
    "print(f\"\")\n",
    "\n",
    "print(f\"üîó S3 Data Location:\")\n",
    "print(f\"   üìä Main dataset: s3://{S3_BUCKET_NAME}/{S3_PREFIX}/processed_data/\")\n",
    "print(f\"   ü§ñ ML chunks: s3://{S3_BUCKET_NAME}/{S3_PREFIX}/emr_ready/fare_prediction_chunks/\")\n",
    "print(f\"   ‚ö° Training script: s3://{S3_BUCKET_NAME}/{S3_PREFIX}/emr_ready/fare_prediction_training.py\")\n",
    "print(f\"   üìà Model output: s3://{S3_BUCKET_NAME}/{S3_PREFIX}/models/ (after training)\")\n",
    "\n",
    "# Alternative: EMR Notebooks approach\n",
    "print(f\"\\nüìì ALTERNATIVE: Use EMR Notebooks for interactive ML development:\")\n",
    "print(f\"1. Create EMR cluster with Jupyter notebooks:\")\n",
    "print(f\"   aws emr create-cluster --name 'NYC-Taxi-Notebooks' --applications Name=Spark Name=JupyterEnterpriseGateway\")\n",
    "print(f\"2. Connect via EMR Notebooks in AWS Console\")\n",
    "print(f\"3. Use the same PySpark code interactively\")\n",
    "\n",
    "print(f\"\\n‚úÖ EMR deployment commands generated!\")\n",
    "print(f\"üöÄ Copy and run these commands to start your AWS MapReduce ML training pipeline!\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
